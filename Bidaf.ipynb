{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bidaf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnK0lzCpvGUW"
      },
      "source": [
        "from torch import nn\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import pickle, time\r\n",
        "import re, os, string, typing, gc, json\r\n",
        "import torch.nn.functional as F\r\n",
        "import spacy\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from collections import Counter\r\n",
        "nlp = spacy.load('en_core_web_sm')\r\n",
        "from spacy.lang.en import English\r\n",
        "# from preprocess import *\r\n",
        "%load_ext autoreload\r\n",
        "%autoreload 2\r\n",
        "import sys\r\n",
        "import csv\r\n",
        "import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BViNLYhW2djh"
      },
      "source": [
        "**Dataset Load**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jta29nlRANPZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ysZaX3Z3bBq"
      },
      "source": [
        "def make_dataframe(path, labels):\r\n",
        "   \r\n",
        "    data = []\r\n",
        "    context = []\r\n",
        "    Answer = []\r\n",
        "    question = []\r\n",
        "    with open(path, 'r', encoding='utf-8') as csv_file:\r\n",
        "        csv_reader = csv.reader(csv_file)\r\n",
        "        line_count = []\r\n",
        "        for row in csv_reader:\r\n",
        "            line_count.append(row)\r\n",
        "    for i in range(len(line_count)):\r\n",
        "      txt = ' '\r\n",
        "      for j in range(len(line_count[i])):\r\n",
        "        txt = txt + line_count[i][j]\r\n",
        "      data.append(txt)\r\n",
        "      txt = txt + '\\n' \r\n",
        "    ##########################################################\r\n",
        "  \r\n",
        "\r\n",
        "    ############################################################\r\n",
        "    label_list = []\r\n",
        "    label_list_final = []\r\n",
        "    final_label = []\r\n",
        "    f = open(labels, \"r\") \r\n",
        "    # print(len(f.readlines()))\r\n",
        "    for i in f.read().split('\\n'):\r\n",
        "      # print(i)\r\n",
        "      s = i.replace('\\t', ',')\r\n",
        "      # print(s)\r\n",
        "      label_list.append(s)\r\n",
        "      label_list_final = label_list\r\n",
        "    # print(label_list_final)\r\n",
        "\r\n",
        "    for i in range(len(label_list_final)):\r\n",
        "        k = label_list_final[i].split(\",\")\r\n",
        "        final_label.append(k)\r\n",
        "\r\n",
        "    \r\n",
        "    list1 = []\r\n",
        "    list2 = []\r\n",
        "    list3 = []\r\n",
        "    for i in range(len(label_list_final)-1):\r\n",
        "      # print(final_label[i])\r\n",
        "      answer_start = final_label[i][0]\r\n",
        "      # print(\"s\",answer_start)\r\n",
        "      answer_end = final_label[i][1]\r\n",
        "      # print(\"e\",answer_end)\r\n",
        "      list2.append([answer_start, answer_end])\r\n",
        "      ####\r\n",
        "      # list1.append(final_label[i])\r\n",
        "      # answer_start = label_list[i][0]\r\n",
        "      # # print(\"ans\", label_list[i])\r\n",
        "      # answer_end = list1[i][1]\r\n",
        "      # print(\"ans\", label_list[i][1])\r\n",
        "      # list2.append([answer_start, answer_end])\r\n",
        "      # list3.append(list2)\r\n",
        "    # print(list2) \r\n",
        "    # print(\"list\",list2[0:3]) \r\n",
        "    ###########################################################\r\n",
        "\r\n",
        "      # Extract the Context Question Answer\r\n",
        "    for i in range(len(line_count)):\r\n",
        "      test = data[i].strip().split('\\t')\r\n",
        "      context.append(test[0])\r\n",
        "      question.append(test[2])\r\n",
        "      Answer.append(test[1])\r\n",
        " \r\n",
        "    \r\n",
        "    \r\n",
        "    ####################################################################\r\n",
        "    #create dictionary\r\n",
        "    qa_dict = {}\r\n",
        "    qa_list = []\r\n",
        "    # qa_dict[''] = id\r\n",
        "    qa_dict['context'] = context\r\n",
        "    qa_dict['question'] = question\r\n",
        "    qa_dict['label'] = list2\r\n",
        "    qa_dict['Answer'] = Answer\r\n",
        "    qa_list.append(qa_dict)\r\n",
        "    #####################################################################\r\n",
        "    vstack_array=np.empty([1,4],dtype='object')\r\n",
        "    for i in range(len(context)):\r\n",
        "      row=[]\r\n",
        "      # print(\"kkkkkk\",list2[i])\r\n",
        "      row.append(context[i])\r\n",
        "      row.append(question[i])\r\n",
        "      row.append(list2[i])\r\n",
        "      row.append(Answer[i])\r\n",
        "      row=np.array(row, dtype='object')\r\n",
        "      vstack_array=np.vstack((vstack_array,row))\r\n",
        "      l1=vstack_array.tolist()     \r\n",
        "    l1=l1[1:]    \r\n",
        "   \r\n",
        "\r\n",
        "\r\n",
        "    # row = []\r\n",
        "    # row1=[]\r\n",
        "    # row.append(context[0])\r\n",
        "    # row.append(question[0])\r\n",
        "    # row.append(list2[0])\r\n",
        "    # row.append(Answer[0])\r\n",
        "    # #print(row)\r\n",
        "    # row=np.array(row, dtype='object')\r\n",
        "    # print(row)\r\n",
        "    # row1.append(context[1])\r\n",
        "    # row1.append(question[1])\r\n",
        "    # row1.append(list2[1])\r\n",
        "    # row1.append(Answer[1])\r\n",
        "    # #print(row1)\r\n",
        "    # row1=np.array(row1, dtype='object')\r\n",
        "    # print(row1)\r\n",
        "    # l=np.vstack((row,row1))\r\n",
        "    # l1=l.tolist()         \r\n",
        "    # print(len(l1))                    \r\n",
        "    dframe = pd.DataFrame(l1,columns = ['context', 'question','label','answer'])\r\n",
        "    # dframe = pd.DataFrame(np.column_stack([context, question,   Answer]), \r\n",
        "    #                             columns=['context','question',  'Answer'])\r\n",
        "    #dframe.head()\r\n",
        "    \r\n",
        "    return dframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "hfTLrU1f3iTL",
        "outputId": "20f28620-88a9-43c0-85a1-e4c55e3de9e8"
      },
      "source": [
        "train_list = make_dataframe(\"/content/drive/MyDrive/Logic/ScholarlyRead/ScholarlyRead/Train.csv\", \"/content/drive/MyDrive/Logic/ScholarlyRead/ScholarlyRead/train_span.txt\")\r\n",
        "test_list = make_dataframe(\"/content/drive/MyDrive/Logic/ScholarlyRead/ScholarlyRead/dev.csv\", \"/content/drive/MyDrive/Logic/ScholarlyRead/ScholarlyRead/dev-span.txt\")\r\n",
        "valid_list = make_dataframe(\"/content/drive/MyDrive/Logic/ScholarlyRead/ScholarlyRead/test.csv\", \"/content/drive/MyDrive/Logic/ScholarlyRead/ScholarlyRead/test-span.txt\")\r\n",
        "test_list.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>label</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Being digitized the communication system chann...</td>\n",
              "      <td>What kind of media are having a severe workloa...</td>\n",
              "      <td>[20, 49]</td>\n",
              "      <td>communication system channels</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Being digitized the communication system chann...</td>\n",
              "      <td>What kind of line of duty communication system...</td>\n",
              "      <td>[63, 78]</td>\n",
              "      <td>severe workload</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Being digitized the communication system chann...</td>\n",
              "      <td>What kind of a number of increases in apparatu...</td>\n",
              "      <td>[129, 144]</td>\n",
              "      <td>digital devices</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Being digitized the communication system chann...</td>\n",
              "      <td>What kind of internet available in the global ...</td>\n",
              "      <td>[193, 204]</td>\n",
              "      <td>4G services</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Being digitized the communication system chann...</td>\n",
              "      <td>Where are the internet and 4G services available?</td>\n",
              "      <td>[222, 234]</td>\n",
              "      <td>global world</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  ...                         answer\n",
              "0  Being digitized the communication system chann...  ...  communication system channels\n",
              "1  Being digitized the communication system chann...  ...                severe workload\n",
              "2  Being digitized the communication system chann...  ...                digital devices\n",
              "3  Being digitized the communication system chann...  ...                    4G services\n",
              "4  Being digitized the communication system chann...  ...                   global world\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "R2Rbqe94PVzQ",
        "outputId": "9a3c54e2-43b1-4783-e294-8d96bec71a72"
      },
      "source": [
        "# train_df = pd.DataFrame(train_list)\r\n",
        "# test_df = pd.DataFrame(test_list)\r\n",
        "# valid_df = pd.DataFrame(valid_list)\r\n",
        "# test_df\r\n",
        "import random\r\n",
        "train_id = random.sample(range(1, 100000), len(train_list) )\r\n",
        "test_id = random.sample(range(1, 100000), len(test_list) )\r\n",
        "valid_id = random.sample(range(1, 100000), len(valid_list) )\r\n",
        "\r\n",
        "train_list['id'] = train_id\r\n",
        "test_list['id'] = test_id\r\n",
        "valid_list['id'] = valid_id\r\n",
        "\r\n",
        "train_list = train_list[0:1]\r\n",
        "valid_list = valid_list[0:1]\r\n",
        "valid_list.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>label</th>\n",
              "      <th>answer</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Multipath TCP MPTCP is one of the leading prot...</td>\n",
              "      <td>What is the name of one of the leading protoco...</td>\n",
              "      <td>[0, 13]</td>\n",
              "      <td>Multipath TCP</td>\n",
              "      <td>38108</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  ...     id\n",
              "0  Multipath TCP MPTCP is one of the leading prot...  ...  38108\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "JeW9GoLA_3RB",
        "outputId": "ad53438a-aa74-4046-a9da-7a75972dbe1d"
      },
      "source": [
        "train_data=train_list\r\n",
        "test_data=test_list\r\n",
        "valid_data=valid_list\r\n",
        "\r\n",
        "train_list.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>label</th>\n",
              "      <th>answer</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Server side socialbot detection approaches can...</td>\n",
              "      <td>What kind of approaches can identify malicious...</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>Server side socialbot</td>\n",
              "      <td>77203</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  ...     id\n",
              "0  Server side socialbot detection approaches can...  ...  77203\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhBwcpHExAGJ"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDBSR9a-xG0A"
      },
      "source": [
        "import sys\r\n",
        "def preprocess_df(df):\r\n",
        "    \r\n",
        "    def to_lower(text):\r\n",
        "        return text.lower()\r\n",
        "\r\n",
        "    df.context = df.context.apply(to_lower)\r\n",
        "    df.question = df.question.apply(to_lower)\r\n",
        "    df.answer = df.answer.apply(to_lower)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkMHJh6I09pb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "2c3ebf94-8cda-4781-9caf-3cbde2944de2"
      },
      "source": [
        "preprocess_df(train_data)\r\n",
        "preprocess_df(valid_data)\r\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>label</th>\n",
              "      <th>answer</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>server side socialbot detection approaches can...</td>\n",
              "      <td>what kind of approaches can identify malicious...</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>server side socialbot</td>\n",
              "      <td>77203</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  ...     id\n",
              "0  server side socialbot detection approaches can...  ...  77203\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l3qaA8hmL-R"
      },
      "source": [
        "def gather_text_for_vocab(dfs:list):\r\n",
        "    '''\r\n",
        "    Gathers text from contexts and questions to build a vocabulary.\r\n",
        "    \r\n",
        "    :param dfs: list of dataframes of SQUAD dataset.\r\n",
        "    :returns: list of contexts and questions\r\n",
        "    '''\r\n",
        "    \r\n",
        "    text = []\r\n",
        "    total = 0\r\n",
        "    for df in dfs:\r\n",
        "        unique_contexts = list(df.context.unique())\r\n",
        "        unique_questions = list(df.question.unique())\r\n",
        "        total += df.context.nunique() + df.question.nunique()\r\n",
        "        text.extend(unique_contexts + unique_questions)\r\n",
        "    \r\n",
        "    assert len(text) == total\r\n",
        "    \r\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RjTMEiwmL03"
      },
      "source": [
        "def build_word_vocab(vocab_text):\r\n",
        "    '''\r\n",
        "    Builds a word-level vocabulary from the given text.\r\n",
        "    \r\n",
        "    :param list vocab_text: list of contexts and questions\r\n",
        "    :returns \r\n",
        "        dict word2idx: word to index mapping of words\r\n",
        "        dict idx2word: integer to word mapping\r\n",
        "        list word_vocab: list of words sorted by frequency\r\n",
        "    '''\r\n",
        "    \r\n",
        "    \r\n",
        "    words = []\r\n",
        "    for sent in vocab_text:\r\n",
        "        for word in nlp(sent, disable=['parser','tagger','ner']):\r\n",
        "            words.append(word.text)\r\n",
        "\r\n",
        "    word_counter = Counter(words)\r\n",
        "    word_vocab = sorted(word_counter, key=word_counter.get, reverse=True)\r\n",
        "    print(f\"raw-vocab: {len(word_vocab)}\")\r\n",
        "    word_vocab.insert(0, '<unk>')\r\n",
        "    word_vocab.insert(1, '<pad>')\r\n",
        "    print(f\"vocab-length: {len(word_vocab)}\")\r\n",
        "    word2idx = {word:idx for idx, word in enumerate(word_vocab)}\r\n",
        "    print(f\"word2idx-length: {len(word2idx)}\")\r\n",
        "    idx2word = {v:k for k,v in word2idx.items()}\r\n",
        "    \r\n",
        "    \r\n",
        "    return word2idx, idx2word, word_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzFWGHLlmLxb"
      },
      "source": [
        "def build_char_vocab(vocab_text):\r\n",
        "    '''\r\n",
        "    Builds a character-level vocabulary from the given text.\r\n",
        "    \r\n",
        "    :param list vocab_text: list of contexts and questions\r\n",
        "    :returns \r\n",
        "        dict char2idx: character to index mapping of words\r\n",
        "        list char_vocab: list of characters sorted by frequency\r\n",
        "    '''\r\n",
        "    \r\n",
        "    chars = []\r\n",
        "    for sent in vocab_text:\r\n",
        "        for ch in sent:\r\n",
        "            chars.append(ch)\r\n",
        "\r\n",
        "    char_counter = Counter(chars)\r\n",
        "    char_vocab = sorted(char_counter, key=char_counter.get, reverse=True)\r\n",
        "    print(f\"raw-char-vocab: {len(char_vocab)}\")\r\n",
        "    high_freq_char = [char for char, count in char_counter.items() if count>=20]\r\n",
        "    char_vocab = list(set(char_vocab).intersection(set(high_freq_char)))\r\n",
        "    print(f\"char-vocab-intersect: {len(char_vocab)}\")\r\n",
        "    char_vocab.insert(0,'<unk>')\r\n",
        "    char_vocab.insert(1,'<pad>')\r\n",
        "    char2idx = {char:idx for idx, char in enumerate(char_vocab)}\r\n",
        "    print(f\"char2idx-length: {len(char2idx)}\")\r\n",
        "    \r\n",
        "    return char2idx, char_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aVuJbbImLrC"
      },
      "source": [
        "def context_to_ids(text, word2idx):\r\n",
        "    '''\r\n",
        "    Converts context text to their respective ids by mapping each word\r\n",
        "    using word2idx. Input text is tokenized using spacy tokenizer first.\r\n",
        "    \r\n",
        "    :param str text: context text to be converted\r\n",
        "    :param dict word2idx: word to id mapping\r\n",
        "\r\n",
        "    :returns list context_ids: list of mapped ids\r\n",
        "    \r\n",
        "    :raises assertion error: sanity check\r\n",
        "    \r\n",
        "    '''\r\n",
        "    \r\n",
        "    context_tokens = [w.text for w in nlp(text, disable=['parser','tagger','ner'])]\r\n",
        "    context_ids = [word2idx[word] for word in context_tokens]\r\n",
        "    \r\n",
        "    assert len(context_ids) == len(context_tokens)\r\n",
        "    return context_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0df09owhmLnd"
      },
      "source": [
        "def question_to_ids(text, word2idx):\r\n",
        "    '''\r\n",
        "    Converts question text to their respective ids by mapping each word\r\n",
        "    using word2idx. Input text is tokenized using spacy tokenizer first.\r\n",
        "    \r\n",
        "    :param str text: question text to be converted\r\n",
        "    :param dict word2idx: word to id mapping\r\n",
        "    :returns list context_ids: list of mapped ids\r\n",
        "    \r\n",
        "    :raises assertion error: sanity check\r\n",
        "    \r\n",
        "    '''\r\n",
        "    \r\n",
        "    question_tokens = [w.text for w in nlp(text, disable=['parser','tagger','ner'])]\r\n",
        "    question_ids = [word2idx[word] for word in question_tokens]\r\n",
        "    \r\n",
        "    assert len(question_ids) == len(question_tokens)\r\n",
        "    return question_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wkop1gvY7Sc"
      },
      "source": [
        "def get_error_indices(df, idx2word):\r\n",
        "    # print('Komal', df.head())\r\n",
        "    start_value_error, end_value_error, assert_error = test_indices(df, idx2word)\r\n",
        "    err_idx = start_value_error + end_value_error + assert_error\r\n",
        "    err_idx = set(err_idx)\r\n",
        "    print(f\"Number of error indices: {len(err_idx)}\")\r\n",
        "    \r\n",
        "    return err_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMR3CkEKY7PT"
      },
      "source": [
        "def test_indices(df, idx2word):\r\n",
        "    '''\r\n",
        "    Performs the tests mentioned above. This method also gets the start and end of the answers\r\n",
        "    with respect to the context_ids for each example.\r\n",
        "    \r\n",
        "    :param dataframe df: SQUAD df\r\n",
        "    :param dict idx2word: inverse mapping of token ids to words\r\n",
        "    :returns\r\n",
        "        list start_value_error: example idx where the start idx is not found in the start spans\r\n",
        "                                of the text\r\n",
        "        list end_value_error: example idx where the end idx is not found in the end spans\r\n",
        "                              of the text\r\n",
        "        list assert_error: examples that fail assertion errors. A majority are due to the above errors\r\n",
        "        \r\n",
        "    '''\r\n",
        "    \r\n",
        "    start_value_error = []\r\n",
        "    end_value_error = []\r\n",
        "    assert_error = []\r\n",
        "    # print(df.head())\r\n",
        "    for index, row in df.iterrows():\r\n",
        "        # print(\"index\", index)\r\n",
        "        # print(\"row\", row)\r\n",
        "\r\n",
        "        answer_tokens = [w.text for  w in nlp(row['answer'], disable=['parser','tagger','ner'])]\r\n",
        "        print(\"komal\",answer_tokens)\r\n",
        "       \r\n",
        "        \r\n",
        "\r\n",
        "        start_token = answer_tokens[0]\r\n",
        "        end_token = answer_tokens[-1]\r\n",
        "        # print(\"s\", start_token)\r\n",
        "        # print(\"e\", end_token)\r\n",
        "        \r\n",
        "        context_span  = [(word.idx, word.idx + len(word.text)) \r\n",
        "                         for word in nlp(row['context'], disable=['parser','tagger','ner'])]\r\n",
        "        print(context_span)\r\n",
        "        \r\n",
        "        starts, ends = zip(*context_span)\r\n",
        "        print(\"starts\", type(starts))\r\n",
        "        print(\"starts11\", type(starts[0]))\r\n",
        "        print(\"ends\", ends)\r\n",
        "\r\n",
        "        answer_start, answer_end = row['label']\r\n",
        "        print(\"row\", row['label'])\r\n",
        "        answer_start = int(answer_start)\r\n",
        "        answer_end = int(answer_end)\r\n",
        "        print(\"answer_start\",type(answer_start))\r\n",
        "        print(\"answer_end\", answer_end)\r\n",
        "\r\n",
        "        try:\r\n",
        "            start_idx = starts.index(answer_start)\r\n",
        "            print(\"start_idx\",start_idx)\r\n",
        "        except:\r\n",
        "            start_value_error.append(index)\r\n",
        "            print(\"except\")\r\n",
        "        try:\r\n",
        "            end_idx  = ends.index(answer_end)\r\n",
        "            print(\"end_idx\", end_idx)\r\n",
        "        except:\r\n",
        "            end_value_error.append(index)\r\n",
        "            print(\"end_value_error\", end_value_error)\r\n",
        "\r\n",
        "        try:\r\n",
        "            assert idx2word[row['context_ids'][start_idx]] == answer_tokens[0]\r\n",
        "            assert idx2word[row['context_ids'][end_idx]] == answer_tokens[-1]\r\n",
        "        except:\r\n",
        "            assert_error.append(index)\r\n",
        "\r\n",
        "\r\n",
        "    return start_value_error, end_value_error, assert_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzIGdeeUSJyY"
      },
      "source": [
        "def index_answer(row, idx2word):\r\n",
        "    '''\r\n",
        "    Takes in a row of the dataframe or one training example and\r\n",
        "    returns a tuple of start and end positions of answer by calculating \r\n",
        "    spans.\r\n",
        "    '''\r\n",
        "    \r\n",
        "    context_span = [(word.idx, word.idx + len(word.text)) for word in nlp(row.context, disable=['parser','tagger','ner'])]\r\n",
        "    starts, ends = zip(*context_span)\r\n",
        "    \r\n",
        "    answer_start, answer_end = row.label\r\n",
        "    answer_start = int(answer_start)\r\n",
        "    answer_end = int(answer_end)\r\n",
        "    start_idx = starts.index(answer_start)\r\n",
        " \r\n",
        "    end_idx  = ends.index(answer_end)\r\n",
        "    \r\n",
        "    ans_toks = [w.text for w in nlp(row.answer,disable=['parser','tagger','ner'])]\r\n",
        "    ans_start = ans_toks[0]\r\n",
        "    ans_end = ans_toks[-1]\r\n",
        "    assert idx2word[row.context_ids[start_idx]] == ans_start\r\n",
        "    assert idx2word[row.context_ids[end_idx]] == ans_end\r\n",
        "    \r\n",
        "    return [start_idx, end_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "cdV2gFHGHk6L",
        "outputId": "0f46e2c6-a0ad-4a86-fada-0ad047d59a36"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>label</th>\n",
              "      <th>answer</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>server side socialbot detection approaches can...</td>\n",
              "      <td>what kind of approaches can identify malicious...</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>server side socialbot</td>\n",
              "      <td>77203</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  ...     id\n",
              "0  server side socialbot detection approaches can...  ...  77203\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NVxWnKu1bEb",
        "outputId": "eae950f0-cf7c-4333-8776-368ebc2fc42b"
      },
      "source": [
        "# gather text to build vocabularies\r\n",
        "\r\n",
        "%time vocab_text = gather_text_for_vocab([train_data, valid_data])\r\n",
        "print(\"Number of sentences in dataset: \", len(vocab_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.19 ms, sys: 0 ns, total: 1.19 ms\n",
            "Wall time: 1.19 ms\n",
            "Number of sentences in dataset:  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2lgEFO6mL68",
        "outputId": "61e5a98b-5918-4229-f27b-ad68cf6e2a00"
      },
      "source": [
        "%time word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)\r\n",
        "print(\"----------------------------------\")\r\n",
        "%time char2idx, char_vocab = build_char_vocab(vocab_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw-vocab: 199\n",
            "vocab-length: 201\n",
            "word2idx-length: 201\n",
            "CPU times: user 9.07 ms, sys: 0 ns, total: 9.07 ms\n",
            "Wall time: 9.43 ms\n",
            "----------------------------------\n",
            "raw-char-vocab: 32\n",
            "char-vocab-intersect: 22\n",
            "char2idx-length: 24\n",
            "CPU times: user 0 ns, sys: 879 µs, total: 879 µs\n",
            "Wall time: 884 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMVlj-3VVidu",
        "outputId": "99396539-7a3f-4dca-bb20-99aabaf3314a"
      },
      "source": [
        "# numericalize context and questions for training and validation set\r\n",
        "\r\n",
        "%time train_data['context_ids'] = train_data.context.apply(context_to_ids, word2idx=word2idx)\r\n",
        "%time valid_data['context_ids'] = valid_data.context.apply(context_to_ids, word2idx=word2idx)\r\n",
        "%time train_data['question_ids'] = train_data.question.apply(question_to_ids, word2idx=word2idx)\r\n",
        "%time valid_data['question_ids'] = valid_data.question.apply(question_to_ids, word2idx=word2idx)\r\n",
        "# print(\"koml\",train_data.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.52 ms, sys: 0 ns, total: 1.52 ms\n",
            "Wall time: 1.53 ms\n",
            "CPU times: user 1.18 ms, sys: 0 ns, total: 1.18 ms\n",
            "Wall time: 1.19 ms\n",
            "CPU times: user 1.12 ms, sys: 4 µs, total: 1.12 ms\n",
            "Wall time: 1.06 ms\n",
            "CPU times: user 1.03 ms, sys: 7 µs, total: 1.03 ms\n",
            "Wall time: 976 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "TdFxQY9NKDjy",
        "outputId": "403181cf-af7e-40f1-821b-4fb5e96f574d"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>label</th>\n",
              "      <th>answer</th>\n",
              "      <th>id</th>\n",
              "      <th>context_ids</th>\n",
              "      <th>question_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>server side socialbot detection approaches can...</td>\n",
              "      <td>what kind of approaches can identify malicious...</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>server side socialbot</td>\n",
              "      <td>77203</td>\n",
              "      <td>[88, 44, 16, 45, 17, 13, 46, 47, 28, 5, 48, 8,...</td>\n",
              "      <td>[69, 162, 4, 17, 13, 46, 47, 28, 5, 48, 8, 49,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  ...                                       question_ids\n",
              "0  server side socialbot detection approaches can...  ...  [69, 162, 4, 17, 13, 46, 47, 28, 5, 48, 8, 49,...\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "up8ZGSBKYqi8",
        "outputId": "b12e3244-f15e-416b-afce-a9ac4f4bd6ed"
      },
      "source": [
        "# get indices with tokenization errors and drop those indices \r\n",
        "\r\n",
        "train_err = get_error_indices(train_data, idx2word)\r\n",
        "valid_err = get_error_indices(valid_data, idx2word)\r\n",
        "\r\n",
        "train_data.drop(train_err, inplace=True)\r\n",
        "valid_data.drop(valid_err, inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "komal ['server', 'side', 'socialbot']\n",
            "[(0, 6), (7, 11), (12, 21), (22, 31), (32, 42), (43, 46), (47, 55), (56, 65), (66, 74), (75, 78), (79, 84), (85, 87), (88, 94), (95, 101), (102, 110), (111, 115), (115, 116), (117, 124), (125, 129), (130, 133), (133, 136), (137, 143), (144, 153), (154, 163), (164, 172), (173, 175), (176, 180), (181, 186), (187, 192), (193, 200), (201, 206), (207, 215), (215, 216), (217, 226), (227, 230), (231, 241), (242, 245), (246, 252), (253, 255), (256, 262), (263, 269), (270, 274), (275, 277), (278, 283), (283, 284), (285, 288), (289, 300), (301, 303), (304, 310), (311, 315), (315, 316), (316, 320), (321, 330), (331, 341), (342, 344), (345, 347), (348, 352), (353, 355), (356, 363), (364, 368), (369, 372), (373, 382), (383, 385), (386, 392), (393, 397), (398, 400), (401, 405), (405, 406), (407, 409), (410, 414), (415, 420), (421, 423), (424, 432), (433, 434), (435, 441), (442, 444), (445, 459), (460, 466), (467, 471), (472, 474), (475, 480), (481, 484), (485, 495), (496, 499), (500, 507), (508, 516), (517, 519), (520, 529), (530, 539), (539, 540), (541, 546), (547, 549), (550, 559), (560, 563), (564, 575), (576, 577), (578, 592), (593, 596), (597, 599), (600, 606), (607, 611), (612, 617), (618, 621), (622, 628), (629, 633), (634, 636), (637, 640), (641, 651), (651, 652), (653, 661), (662, 664), (665, 672), (673, 674), (675, 684), (685, 694), (695, 702), (703, 714), (715, 723), (724, 728), (729, 739), (740, 748), (749, 751), (752, 759), (760, 768), (768, 769), (770, 772), (773, 777), (778, 781), (782, 785), (786, 797), (798, 806), (807, 813), (814, 816), (817, 824), (825, 828), (829, 836), (837, 841), (842, 844), (845, 851), (852, 856), (857, 859), (860, 867), (867, 868), (869, 871), (872, 880), (881, 883), (884, 892), (893, 894), (895, 898), (899, 907), (908, 910), (911, 918), (919, 926), (927, 933), (934, 938), (939, 941), (942, 945), (946, 950), (950, 951), (952, 955), (956, 969), (970, 972), (973, 977), (978, 983), (984, 987), (988, 997), (997, 998), (999, 1000), (1001, 1004), (1005, 1013), (1014, 1022), (1023, 1026), (1027, 1029), (1030, 1034), (1035, 1038), (1039, 1048), (1049, 1055), (1056, 1065), (1066, 1069), (1070, 1076), (1077, 1081), (1082, 1084), (1085, 1088), (1089, 1095), (1095, 1096), (1097, 1099), (1100, 1102), (1103, 1110), (1111, 1118), (1119, 1127), (1128, 1136), (1137, 1139), (1140, 1146), (1147, 1151), (1152, 1154), (1155, 1160), (1161, 1170), (1171, 1178), (1179, 1183), (1184, 1191), (1192, 1197), (1198, 1204), (1205, 1209), (1210, 1221), (1222, 1226), (1227, 1230), (1231, 1238), (1239, 1246), (1247, 1250), (1251, 1254), (1255, 1261), (1262, 1267), (1268, 1271), (1272, 1277), (1278, 1284), (1285, 1289), (1290, 1297), (1298, 1300), (1301, 1309), (1310, 1313), (1314, 1321), (1322, 1333), (1334, 1336), (1337, 1343), (1344, 1348), (1349, 1356), (1357, 1359), (1360, 1365), (1365, 1366), (1367, 1372), (1373, 1381), (1382, 1385), (1386, 1388), (1389, 1393), (1394, 1396), (1397, 1404), (1405, 1407), (1408, 1414), (1415, 1425), (1426, 1428), (1429, 1440), (1441, 1447), (1448, 1452), (1453, 1455), (1456, 1457), (1458, 1462), (1462, 1463), (1464, 1467), (1468, 1471), (1472, 1480), (1481, 1490), (1491, 1499), (1500, 1503), (1504, 1515), (1516, 1527), (1528, 1535), (1536, 1537), (1538, 1547), (1548, 1551), (1552, 1553), (1554, 1560), (1561, 1572), (1573, 1575), (1576, 1579), (1580, 1585), (1585, 1586)]\n",
            "starts <class 'tuple'>\n",
            "starts11 <class 'int'>\n",
            "ends (6, 11, 21, 31, 42, 46, 55, 65, 74, 78, 84, 87, 94, 101, 110, 115, 116, 124, 129, 133, 136, 143, 153, 163, 172, 175, 180, 186, 192, 200, 206, 215, 216, 226, 230, 241, 245, 252, 255, 262, 269, 274, 277, 283, 284, 288, 300, 303, 310, 315, 316, 320, 330, 341, 344, 347, 352, 355, 363, 368, 372, 382, 385, 392, 397, 400, 405, 406, 409, 414, 420, 423, 432, 434, 441, 444, 459, 466, 471, 474, 480, 484, 495, 499, 507, 516, 519, 529, 539, 540, 546, 549, 559, 563, 575, 577, 592, 596, 599, 606, 611, 617, 621, 628, 633, 636, 640, 651, 652, 661, 664, 672, 674, 684, 694, 702, 714, 723, 728, 739, 748, 751, 759, 768, 769, 772, 777, 781, 785, 797, 806, 813, 816, 824, 828, 836, 841, 844, 851, 856, 859, 867, 868, 871, 880, 883, 892, 894, 898, 907, 910, 918, 926, 933, 938, 941, 945, 950, 951, 955, 969, 972, 977, 983, 987, 997, 998, 1000, 1004, 1013, 1022, 1026, 1029, 1034, 1038, 1048, 1055, 1065, 1069, 1076, 1081, 1084, 1088, 1095, 1096, 1099, 1102, 1110, 1118, 1127, 1136, 1139, 1146, 1151, 1154, 1160, 1170, 1178, 1183, 1191, 1197, 1204, 1209, 1221, 1226, 1230, 1238, 1246, 1250, 1254, 1261, 1267, 1271, 1277, 1284, 1289, 1297, 1300, 1309, 1313, 1321, 1333, 1336, 1343, 1348, 1356, 1359, 1365, 1366, 1372, 1381, 1385, 1388, 1393, 1396, 1404, 1407, 1414, 1425, 1428, 1440, 1447, 1452, 1455, 1457, 1462, 1463, 1467, 1471, 1480, 1490, 1499, 1503, 1515, 1527, 1535, 1537, 1547, 1551, 1553, 1560, 1572, 1575, 1579, 1585, 1586)\n",
            "row ['0', '21']\n",
            "answer_start <class 'int'>\n",
            "answer_end 21\n",
            "start_idx 0\n",
            "end_idx 2\n",
            "Number of error indices: 0\n",
            "komal ['multipath', 'tcp']\n",
            "[(0, 9), (10, 13), (14, 19), (20, 22), (23, 26), (27, 29), (30, 33), (34, 41), (42, 51), (52, 54), (55, 62), (63, 72), (73, 82), (83, 85), (86, 87), (88, 97), (98, 103), (103, 104), (105, 112), (113, 122), (123, 125), (126, 129), (130, 138), (139, 142), (143, 146), (147, 158), (159, 165), (166, 169), (170, 178), (179, 184), (185, 188), (189, 199), (200, 210), (211, 222), (222, 223), (224, 229), (230, 243), (244, 251), (252, 260), (261, 269), (270, 272), (273, 280), (281, 284), (284, 285), (286, 288), (289, 293), (294, 299), (300, 302), (303, 310), (311, 312), (313, 316), (317, 327), (328, 334), (335, 338), (339, 344), (345, 349), (350, 358), (359, 365), (366, 376), (377, 386), (387, 389), (390, 393), (394, 402), (403, 409), (410, 413), (414, 421), (422, 427), (427, 428), (429, 432), (433, 439), (440, 449), (450, 453), (453, 454), (454, 456), (456, 457), (457, 462), (463, 470), (471, 480), (481, 483), (484, 495), (496, 507), (508, 515), (516, 524), (525, 528), (529, 536), (537, 541), (542, 549), (550, 552), (553, 561), (562, 564), (565, 574), (575, 578), (579, 588), (589, 592), (592, 593), (593, 595), (595, 596), (596, 601), (602, 609), (610, 613), (614, 617), (618, 624), (625, 629), (629, 630), (631, 633), (634, 643), (644, 647), (648, 656), (657, 667), (668, 670), (671, 674), (675, 680), (681, 687), (688, 691), (692, 694), (695, 703), (704, 707), (708, 719), (720, 724), (725, 726), (727, 734), (735, 742), (743, 752), (753, 758), (759, 763), (764, 767), (768, 772), (773, 781), (781, 782), (783, 786), (787, 794), (795, 799), (800, 804), (805, 808), (809, 817), (818, 828), (829, 835), (836, 844), (845, 854), (855, 861), (862, 874), (875, 885), (886, 888), (889, 892), (893, 904), (905, 916), (917, 919), (920, 928), (929, 934), (935, 938), (939, 945), (946, 950), (950, 951), (952, 960), (961, 964), (965, 973), (974, 984), (985, 988), (989, 999), (1000, 1003), (1004, 1013), (1014, 1018), (1019, 1021), (1022, 1030), (1031, 1039), (1040, 1044), (1045, 1047), (1048, 1051), (1052, 1053), (1054, 1064), (1064, 1065), (1065, 1070), (1071, 1079), (1079, 1080)]\n",
            "starts <class 'tuple'>\n",
            "starts11 <class 'int'>\n",
            "ends (9, 13, 19, 22, 26, 29, 33, 41, 51, 54, 62, 72, 82, 85, 87, 97, 103, 104, 112, 122, 125, 129, 138, 142, 146, 158, 165, 169, 178, 184, 188, 199, 210, 222, 223, 229, 243, 251, 260, 269, 272, 280, 284, 285, 288, 293, 299, 302, 310, 312, 316, 327, 334, 338, 344, 349, 358, 365, 376, 386, 389, 393, 402, 409, 413, 421, 427, 428, 432, 439, 449, 453, 454, 456, 457, 462, 470, 480, 483, 495, 507, 515, 524, 528, 536, 541, 549, 552, 561, 564, 574, 578, 588, 592, 593, 595, 596, 601, 609, 613, 617, 624, 629, 630, 633, 643, 647, 656, 667, 670, 674, 680, 687, 691, 694, 703, 707, 719, 724, 726, 734, 742, 752, 758, 763, 767, 772, 781, 782, 786, 794, 799, 804, 808, 817, 828, 835, 844, 854, 861, 874, 885, 888, 892, 904, 916, 919, 928, 934, 938, 945, 950, 951, 960, 964, 973, 984, 988, 999, 1003, 1013, 1018, 1021, 1030, 1039, 1044, 1047, 1051, 1053, 1064, 1065, 1070, 1079, 1080)\n",
            "row ['0', '13']\n",
            "answer_start <class 'int'>\n",
            "answer_end 13\n",
            "start_idx 0\n",
            "end_idx 1\n",
            "Number of error indices: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "Hoghg6YzH8Me",
        "outputId": "6d3ec3d1-6c19-4ce0-8dd4-7069ef09ccd1"
      },
      "source": [
        "valid_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>label</th>\n",
              "      <th>answer</th>\n",
              "      <th>id</th>\n",
              "      <th>context_ids</th>\n",
              "      <th>question_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>multipath tcp mptcp is one of the leading prot...</td>\n",
              "      <td>what is the name of one of the leading protoco...</td>\n",
              "      <td>[0, 13]</td>\n",
              "      <td>multipath tcp</td>\n",
              "      <td>38108</td>\n",
              "      <td>[39, 71, 26, 24, 72, 4, 2, 73, 74, 7, 75, 39, ...</td>\n",
              "      <td>[69, 24, 2, 200, 4, 72, 4, 2, 73, 74, 7, 75, 3...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  ...                                       question_ids\n",
              "0  multipath tcp mptcp is one of the leading prot...  ...  [69, 24, 2, 200, 4, 72, 4, 2, 73, 74, 7, 75, 3...\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "xeQ25olaxRmX",
        "outputId": "99e64067-6d7d-4a7b-c121-cc0eb2bcf282"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>label</th>\n",
              "      <th>answer</th>\n",
              "      <th>id</th>\n",
              "      <th>context_ids</th>\n",
              "      <th>question_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>server side socialbot detection approaches can...</td>\n",
              "      <td>what kind of approaches can identify malicious...</td>\n",
              "      <td>[0, 21]</td>\n",
              "      <td>server side socialbot</td>\n",
              "      <td>77203</td>\n",
              "      <td>[88, 44, 16, 45, 17, 13, 46, 47, 28, 5, 48, 8,...</td>\n",
              "      <td>[69, 162, 4, 17, 13, 46, 47, 28, 5, 48, 8, 49,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  ...                                       question_ids\n",
              "0  server side socialbot detection approaches can...  ...  [69, 162, 4, 17, 13, 46, 47, 28, 5, 48, 8, 49,...\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czREMC-FY7J7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f109aab-593a-47a6-b324-295ca4f7cd99"
      },
      "source": [
        "# get start and end positions of answers from the context\r\n",
        "# this is basically the label for training QA models\r\n",
        "\r\n",
        "train_label_idx = train_data.apply(index_answer, axis=1, idx2word=idx2word)\r\n",
        "valid_label_idx = valid_data.apply(index_answer, axis=1, idx2word=idx2word)\r\n",
        "print(train_label_idx)\r\n",
        "\r\n",
        "train_data['label_idx'] = train_label_idx\r\n",
        "valid_data['label_idx'] = valid_label_idx\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    [0, 2]\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrWwX3AYSCxC"
      },
      "source": [
        "# dump to pickle files\r\n",
        "\r\n",
        "train_data.to_pickle('bidaftrain.pkl')\r\n",
        "valid_data.to_pickle('bidafvalid.pkl')\r\n",
        "\r\n",
        "with open('qanetw2id.pickle','wb') as handle:\r\n",
        "    pickle.dump(word2idx, handle)\r\n",
        "\r\n",
        "with open('qanetc2id.pickle','wb') as handle:\r\n",
        "    pickle.dump(char2idx, handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_LRzPLvyb8k"
      },
      "source": [
        "# load data from pickle files\r\n",
        "\r\n",
        "\r\n",
        "train_data = pd.read_pickle('bidaftrain.pkl')\r\n",
        "valid_data = pd.read_pickle('bidafvalid.pkl')\r\n",
        "\r\n",
        "with open('qanetw2id.pickle','rb') as handle:\r\n",
        "    word2idx = pickle.load(handle)\r\n",
        "with open('qanetc2id.pickle','rb') as handle:\r\n",
        "    char2idx = pickle.load(handle)\r\n",
        "\r\n",
        "idx2word = {v:k for k,v in word2idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLZ0SzmZyb5M"
      },
      "source": [
        "class SquadDataset:\r\n",
        "    '''\r\n",
        "    - Creates batches dynamically by padding to the length of largest example\r\n",
        "      in a given batch.\r\n",
        "    - Calulates character vectors for contexts and question.\r\n",
        "    - Returns tensors for training.\r\n",
        "    '''\r\n",
        "    \r\n",
        "    def __init__(self, data, batch_size):\r\n",
        "        \r\n",
        "        self.batch_size = batch_size\r\n",
        "        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\r\n",
        "        self.data = data\r\n",
        "        \r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data)\r\n",
        "    \r\n",
        "    def make_char_vector(self, max_sent_len, max_word_len, sentence):\r\n",
        "        \r\n",
        "        char_vec = torch.ones(max_sent_len, max_word_len).type(torch.LongTensor)\r\n",
        "        \r\n",
        "        for i, word in enumerate(nlp(sentence, disable=['parser','tagger','ner'])):\r\n",
        "            for j, ch in enumerate(word.text):\r\n",
        "                char_vec[i][j] = char2idx.get(ch, 0)\r\n",
        "        \r\n",
        "        return char_vec    \r\n",
        "    \r\n",
        "    def get_span(self, text):\r\n",
        "        \r\n",
        "        text = nlp(text, disable=['parser','tagger','ner'])\r\n",
        "        span = [(w.idx, w.idx+len(w.text)) for w in text]\r\n",
        "\r\n",
        "        return span\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        '''\r\n",
        "        Creates batches of data and yields them.\r\n",
        "        \r\n",
        "        Each yield comprises of:\r\n",
        "        :padded_context: padded tensor of contexts for each batch \r\n",
        "        :padded_question: padded tensor of questions for each batch \r\n",
        "        :char_ctx & ques_ctx: character-level ids for context and question\r\n",
        "        :label: start and end index wrt context_ids\r\n",
        "        :context_text,answer_text: used while validation to calculate metrics\r\n",
        "        :ids: question_ids for evaluation\r\n",
        "        \r\n",
        "        '''\r\n",
        "        \r\n",
        "        for batch in self.data:\r\n",
        "            \r\n",
        "            spans = []\r\n",
        "            ctx_text = []\r\n",
        "            answer_text = []\r\n",
        "            \r\n",
        "            for ctx in batch.context:\r\n",
        "                ctx_text.append(ctx)\r\n",
        "                spans.append(self.get_span(ctx))\r\n",
        "            \r\n",
        "            for ans in batch.answer:\r\n",
        "                answer_text.append(ans)\r\n",
        "                \r\n",
        "            \r\n",
        "            max_context_len = max([len(ctx) for ctx in batch.context_ids])\r\n",
        "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\r\n",
        "            \r\n",
        "            for i, ctx in enumerate(batch.context_ids):\r\n",
        "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\r\n",
        "                \r\n",
        "            max_word_ctx = 0\r\n",
        "            for context in batch.context:\r\n",
        "                for word in nlp(context, disable=['parser','tagger','ner']):\r\n",
        "                    if len(word.text) > max_word_ctx:\r\n",
        "                        max_word_ctx = len(word.text)\r\n",
        "            \r\n",
        "            char_ctx = torch.ones(len(batch), max_context_len, max_word_ctx).type(torch.LongTensor)\r\n",
        "            for i, context in enumerate(batch.context):\r\n",
        "                char_ctx[i] = self.make_char_vector(max_context_len, max_word_ctx, context)\r\n",
        "            \r\n",
        "            max_question_len = max([len(ques) for ques in batch.question_ids])\r\n",
        "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\r\n",
        "            \r\n",
        "            for i, ques in enumerate(batch.question_ids):\r\n",
        "                padded_question[i, :len(ques)] = torch.LongTensor(ques)\r\n",
        "                \r\n",
        "            max_word_ques = 0\r\n",
        "            for question in batch.question:\r\n",
        "                for word in nlp(question, disable=['parser','tagger','ner']):\r\n",
        "                    if len(word.text) > max_word_ques:\r\n",
        "                        max_word_ques = len(word.text)\r\n",
        "            \r\n",
        "            char_ques = torch.ones(len(batch), max_question_len, max_word_ques).type(torch.LongTensor)\r\n",
        "            for i, question in enumerate(batch.question):\r\n",
        "                char_ques[i] = self.make_char_vector(max_question_len, max_word_ques, question)\r\n",
        "            \r\n",
        "            ids = list(batch.id)  \r\n",
        "            label = torch.LongTensor(list(batch.label_idx))\r\n",
        "            \r\n",
        "            yield (padded_context, padded_question, char_ctx, char_ques, label, ctx_text, answer_text, ids)\r\n",
        "            \r\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9necV2J7ooBY"
      },
      "source": [
        "def p(self, b=None):\r\n",
        "    if b is None:\r\n",
        "        b = self.a\r\n",
        "    print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J01OFIPyb2u"
      },
      "source": [
        "train_dataset = SquadDataset(train_data, 16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A67eRRcY7Gj"
      },
      "source": [
        "valid_dataset = SquadDataset(valid_data, 16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ1xhtd0Y7Eb"
      },
      "source": [
        "a = next(iter(train_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5GnHq12hzaM"
      },
      "source": [
        "weights_matrix = np.load('/content/drive/MyDrive/Logic/bidafglove_tv.npy')\r\n",
        "num_embeddings, embedding_dim = weights_matrix.shape\r\n",
        "#embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\r\n",
        "embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix),freeze=True)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxXiZFfz6jRi"
      },
      "source": [
        "def get_glove_dict():\r\n",
        "    '''\r\n",
        "    Parses the glove word vectors text file and returns a dictionary with the words as\r\n",
        "    keys and their respective pretrained word vectors as values.\r\n",
        "\r\n",
        "    '''\r\n",
        "    glove_dict = {}\r\n",
        "    with open(\"/content/drive/MyDrive/Logic/glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "        for line in f:\r\n",
        "            values = line.split()\r\n",
        "            word = values[0]\r\n",
        "            vector = np.asarray(values[1:], \"float32\")\r\n",
        "            glove_dict[word] = vector\r\n",
        "            \r\n",
        "    f.close()\r\n",
        "    \r\n",
        "    return glove_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a27SHUbV6jNr"
      },
      "source": [
        "glove_dict = get_glove_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcT8HQ6Ag7_Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XDpJzMA6jLl"
      },
      "source": [
        "def create_weights_matrix(glove_dict):\r\n",
        "    '''\r\n",
        "    Creates a weight matrix of the words that are common in the GloVe vocab and\r\n",
        "    the dataset's vocab. Initializes OOV words with a zero vector.\r\n",
        "    '''\r\n",
        "    weights_matrix = np.zeros((len(word_vocab), 100))\r\n",
        "    words_found = 0\r\n",
        "    for i, word in enumerate(word_vocab):\r\n",
        "        try:\r\n",
        "            weights_matrix[i] = glove_dict[word]\r\n",
        "            words_found += 1\r\n",
        "        except:\r\n",
        "            pass\r\n",
        "        \r\n",
        "    return weights_matrix, words_found\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lnjWoOOLlOS",
        "outputId": "bbcfa5f3-a935-4bcc-972d-6c90cf5dfc98"
      },
      "source": [
        "weights_matrix, words_found = create_weights_matrix(glove_dict)\r\n",
        "print(\"Words found in the GloVe vocab: \" ,words_found)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words found in the GloVe vocab:  193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3drHgoFewK9"
      },
      "source": [
        "# dump the weights to load in future\r\n",
        "\r\n",
        "np.save('/content/drive/MyDrive/Logic/bidafglove_tv.npy', weights_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKT99QZXewHm"
      },
      "source": [
        "class CharacterEmbeddingLayer(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self, char_vocab_dim, char_emb_dim, num_output_channels, kernel_size):\r\n",
        "        \r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.char_emb_dim = char_emb_dim\r\n",
        "        \r\n",
        "        self.char_embedding = nn.Embedding(char_vocab_dim, char_emb_dim, padding_idx=1)\r\n",
        "        \r\n",
        "        self.char_convolution = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=kernel_size)\r\n",
        "        \r\n",
        "        self.relu = nn.ReLU()\r\n",
        "    \r\n",
        "        self.dropout = nn.Dropout(0.2)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        # x = [bs, seq_len, word_len]\r\n",
        "        # returns : [batch_size, seq_len, num_output_channels]\r\n",
        "        # the output can be thought of as another feature embedding of dim 100.\r\n",
        "        \r\n",
        "        batch_size = x.shape[0]\r\n",
        "        \r\n",
        "        x = self.dropout(self.char_embedding(x))\r\n",
        "        # x = [bs, seq_len, word_len, char_emb_dim]\r\n",
        "        \r\n",
        "        # following three operations manipulate x in such a way that\r\n",
        "        # it closely resembles an image. this format is important before \r\n",
        "        # we perform convolution on the character embeddings.\r\n",
        "        \r\n",
        "        x = x.permute(0,1,3,2)\r\n",
        "        # x = [bs, seq_len, char_emb_dim, word_len]\r\n",
        "        \r\n",
        "        x = x.view(-1, self.char_emb_dim, x.shape[3])\r\n",
        "        # x = [bs*seq_len, char_emb_dim, word_len]\r\n",
        "        \r\n",
        "        x = x.unsqueeze(1)\r\n",
        "        # x = [bs*seq_len, 1, char_emb_dim, word_len]\r\n",
        "        \r\n",
        "        # x is now in a format that can be accepted by a conv layer. \r\n",
        "        # think of the tensor above in terms of an image of dimension\r\n",
        "        # (N, C_in, H_in, W_in).\r\n",
        "        \r\n",
        "        x = self.relu(self.char_convolution(x))\r\n",
        "        # x = [bs*seq_len, out_channels, H_out, W_out]\r\n",
        "        \r\n",
        "        x = x.squeeze()\r\n",
        "        # x = [bs*seq_len, out_channels, W_out]\r\n",
        "                \r\n",
        "        x = F.max_pool1d(x, x.shape[2]).squeeze()\r\n",
        "        # x = [bs*seq_len, out_channels, 1] => [bs*seq_len, out_channels]\r\n",
        "        \r\n",
        "        x = x.view(batch_size, -1, x.shape[-1])\r\n",
        "        # x = [bs, seq_len, out_channels]\r\n",
        "        # x = [bs, seq_len, features] = [bs, seq_len, 100]\r\n",
        "        \r\n",
        "        \r\n",
        "        return x        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10SW-bpaewE5"
      },
      "source": [
        "class HighwayNetwork(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self, input_dim, num_layers=2):\r\n",
        "        \r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.num_layers = num_layers\r\n",
        "        \r\n",
        "        self.flow_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\r\n",
        "        self.gate_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        for i in range(self.num_layers):\r\n",
        "            \r\n",
        "            flow_value = F.relu(self.flow_layer[i](x))\r\n",
        "            gate_value = torch.sigmoid(self.gate_layer[i](x))\r\n",
        "            \r\n",
        "            x = gate_value * flow_value + (1-gate_value) * x\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEeHpkvgewBh"
      },
      "source": [
        "class ContextualEmbeddingLayer(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self, input_dim, hidden_dim):\r\n",
        "        \r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\r\n",
        "        \r\n",
        "        self.highway_net = HighwayNetwork(input_dim)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        # x = [bs, seq_len, input_dim] = [bs, seq_len, emb_dim*2]\r\n",
        "        # the input is the concatenation of word and characeter embeddings\r\n",
        "        # for the sequence.\r\n",
        "        \r\n",
        "        highway_out = self.highway_net(x)\r\n",
        "        # highway_out = [bs, seq_len, input_dim]\r\n",
        "        \r\n",
        "        outputs, _ = self.lstm(highway_out)\r\n",
        "        # outputs = [bs, seq_len, emb_dim*2]\r\n",
        "        \r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF3C6dgOev-q"
      },
      "source": [
        "class BiDAF(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self, char_vocab_dim, emb_dim, char_emb_dim, num_output_channels, \r\n",
        "                 kernel_size, ctx_hidden_dim, device):\r\n",
        "        '''\r\n",
        "        char_vocab_dim = len(char2idx)\r\n",
        "        emb_dim = 100\r\n",
        "        char_emb_dim = 8\r\n",
        "        num_output_chanels = 100\r\n",
        "        kernel_size = (8,5)\r\n",
        "        ctx_hidden_dim = 100\r\n",
        "        '''\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.word_embedding = self.get_glove_embedding()\r\n",
        "        \r\n",
        "        self.character_embedding = CharacterEmbeddingLayer(char_vocab_dim, char_emb_dim, \r\n",
        "                                                      num_output_channels, kernel_size)\r\n",
        "        \r\n",
        "        self.contextual_embedding = ContextualEmbeddingLayer(emb_dim*2, ctx_hidden_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout()\r\n",
        "        \r\n",
        "        self.similarity_weight = nn.Linear(emb_dim*6, 1, bias=False)\r\n",
        "        \r\n",
        "        self.modeling_lstm = nn.LSTM(emb_dim*8, emb_dim, bidirectional=True, num_layers=2, batch_first=True, dropout=0.2)\r\n",
        "        \r\n",
        "        self.output_start = nn.Linear(emb_dim*10, 1, bias=False)\r\n",
        "        \r\n",
        "        self.output_end = nn.Linear(emb_dim*10, 1, bias=False)\r\n",
        "        \r\n",
        "        self.end_lstm = nn.LSTM(emb_dim*2, emb_dim, bidirectional=True, batch_first=True)\r\n",
        "        \r\n",
        "    \r\n",
        "    def get_glove_embedding(self):\r\n",
        "        \r\n",
        "        weights_matrix = np.load('/content/drive/MyDrive/Logic/bidafglove_tv.npy')\r\n",
        "        num_embeddings, embedding_dim = weights_matrix.shape\r\n",
        "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\r\n",
        "\r\n",
        "        return embedding\r\n",
        "        \r\n",
        "    def forward(self, ctx, ques, char_ctx, char_ques):\r\n",
        "        # ctx = [bs, ctx_len]\r\n",
        "        # ques = [bs, ques_len]\r\n",
        "        # char_ctx = [bs, ctx_len, ctx_word_len]\r\n",
        "        # char_ques = [bs, ques_len, ques_word_len]\r\n",
        "        \r\n",
        "        ctx_len = ctx.shape[1]\r\n",
        "        \r\n",
        "        ques_len = ques.shape[1]\r\n",
        "        \r\n",
        "        ## GET WORD AND CHARACTER EMBEDDINGS\r\n",
        "        \r\n",
        "        ctx_word_embed = self.word_embedding(ctx)\r\n",
        "        # ctx_word_embed = [bs, ctx_len, emb_dim]\r\n",
        "        \r\n",
        "        ques_word_embed = self.word_embedding(ques)\r\n",
        "        # ques_word_embed = [bs, ques_len, emb_dim]\r\n",
        "        \r\n",
        "        ctx_char_embed = self.character_embedding(char_ctx)\r\n",
        "        # ctx_char_embed =  [bs, ctx_len, emb_dim]\r\n",
        "        \r\n",
        "        ques_char_embed = self.character_embedding(char_ques)\r\n",
        "        # ques_char_embed = [bs, ques_len, emb_dim]\r\n",
        "        \r\n",
        "        ## CREATE CONTEXTUAL EMBEDDING\r\n",
        "        \r\n",
        "        ctx_contextual_inp = torch.cat([ctx_word_embed, ctx_char_embed],dim=2)\r\n",
        "        # [bs, ctx_len, emb_dim*2]\r\n",
        "        \r\n",
        "        ques_contextual_inp = torch.cat([ques_word_embed, ques_char_embed],dim=2)\r\n",
        "        # [bs, ques_len, emb_dim*2]\r\n",
        "        \r\n",
        "        ctx_contextual_emb = self.contextual_embedding(ctx_contextual_inp)\r\n",
        "        # [bs, ctx_len, emb_dim*2]\r\n",
        "        \r\n",
        "        ques_contextual_emb = self.contextual_embedding(ques_contextual_inp)\r\n",
        "        # [bs, ques_len, emb_dim*2]\r\n",
        "        \r\n",
        "        \r\n",
        "        ## CREATE SIMILARITY MATRIX\r\n",
        "        \r\n",
        "        ctx_ = ctx_contextual_emb.unsqueeze(2).repeat(1,1,ques_len,1)\r\n",
        "        # [bs, ctx_len, 1, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\r\n",
        "        \r\n",
        "        ques_ = ques_contextual_emb.unsqueeze(1).repeat(1,ctx_len,1,1)\r\n",
        "        # [bs, 1, ques_len, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\r\n",
        "        \r\n",
        "        elementwise_prod = torch.mul(ctx_, ques_)\r\n",
        "        # [bs, ctx_len, ques_len, emb_dim*2]\r\n",
        "        \r\n",
        "        alpha = torch.cat([ctx_, ques_, elementwise_prod], dim=3)\r\n",
        "        # [bs, ctx_len, ques_len, emb_dim*6]\r\n",
        "        \r\n",
        "        similarity_matrix = self.similarity_weight(alpha).view(-1, ctx_len, ques_len)\r\n",
        "        # [bs, ctx_len, ques_len]\r\n",
        "        \r\n",
        "        \r\n",
        "        ## CALCULATE CONTEXT2QUERY ATTENTION\r\n",
        "        \r\n",
        "        a = F.softmax(similarity_matrix, dim=-1)\r\n",
        "        # [bs, ctx_len, ques_len]\r\n",
        "        \r\n",
        "        c2q = torch.bmm(a, ques_contextual_emb)\r\n",
        "        # [bs] ([ctx_len, ques_len] X [ques_len, emb_dim*2]) => [bs, ctx_len, emb_dim*2]\r\n",
        "        \r\n",
        "        \r\n",
        "        ## CALCULATE QUERY2CONTEXT ATTENTION\r\n",
        "        \r\n",
        "        b = F.softmax(torch.max(similarity_matrix,2)[0], dim=-1)\r\n",
        "        # [bs, ctx_len]\r\n",
        "        \r\n",
        "        b = b.unsqueeze(1)\r\n",
        "        # [bs, 1, ctx_len]\r\n",
        "        \r\n",
        "        q2c = torch.bmm(b, ctx_contextual_emb)\r\n",
        "        # [bs] ([bs, 1, ctx_len] X [bs, ctx_len, emb_dim*2]) => [bs, 1, emb_dim*2]\r\n",
        "        \r\n",
        "        q2c = q2c.repeat(1, ctx_len, 1)\r\n",
        "        # [bs, ctx_len, emb_dim*2]\r\n",
        "        \r\n",
        "        ## QUERY AWARE REPRESENTATION\r\n",
        "        \r\n",
        "        G = torch.cat([ctx_contextual_emb, c2q, \r\n",
        "                       torch.mul(ctx_contextual_emb,c2q), \r\n",
        "                       torch.mul(ctx_contextual_emb, q2c)], dim=2)\r\n",
        "        \r\n",
        "        # [bs, ctx_len, emb_dim*8]\r\n",
        "        \r\n",
        "        \r\n",
        "        ## MODELING LAYER\r\n",
        "        \r\n",
        "        M, _ = self.modeling_lstm(G)\r\n",
        "        # [bs, ctx_len, emb_dim*2]\r\n",
        "        \r\n",
        "        ## OUTPUT LAYER\r\n",
        "        \r\n",
        "        M2, _ = self.end_lstm(M)\r\n",
        "        \r\n",
        "        # START PREDICTION\r\n",
        "        \r\n",
        "        p1 = self.output_start(torch.cat([G,M], dim=2))\r\n",
        "        # [bs, ctx_len, 1]\r\n",
        "        \r\n",
        "        p1 = p1.squeeze()\r\n",
        "        # [bs, ctx_len]\r\n",
        "        \r\n",
        "        #p1 = F.softmax(p1, dim=-1)\r\n",
        "        \r\n",
        "        # END PREDICTION\r\n",
        "        \r\n",
        "        p2 = self.output_end(torch.cat([G, M2], dim=2)).squeeze()\r\n",
        "        # [bs, ctx_len, 1] => [bs, ctx_len]\r\n",
        "        \r\n",
        "        #p2 = F.softmax(p2, dim=-1)\r\n",
        "        \r\n",
        "        \r\n",
        "        return p1, p2\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diEkcvCuLlKs"
      },
      "source": [
        "CHAR_VOCAB_DIM = len(char2idx)\r\n",
        "EMB_DIM = 100\r\n",
        "CHAR_EMB_DIM = 8\r\n",
        "NUM_OUTPUT_CHANNELS = 100\r\n",
        "KERNEL_SIZE = (8,5)\r\n",
        "HIDDEN_DIM = 100\r\n",
        "device = torch.device('cuda')\r\n",
        "\r\n",
        "model = BiDAF(CHAR_VOCAB_DIM, \r\n",
        "              EMB_DIM, \r\n",
        "              CHAR_EMB_DIM, \r\n",
        "              NUM_OUTPUT_CHANNELS, \r\n",
        "              KERNEL_SIZE, \r\n",
        "              HIDDEN_DIM, \r\n",
        "              device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPbSfPDPLlIL"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "from torch.autograd import Variable\r\n",
        "optimizer = optim.Adadelta(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCOoC-PwLlFi"
      },
      "source": [
        "def train(model, train_dataset):\r\n",
        "    print(\"Starting training ........\")\r\n",
        "   \r\n",
        "\r\n",
        "    train_loss = 0.\r\n",
        "    batch_count = 0\r\n",
        "    model.train()\r\n",
        "    for batch in train_dataset:\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "    \r\n",
        "        if batch_count % 500 == 0:\r\n",
        "            print(f\"Starting batch: {batch_count}\")\r\n",
        "        batch_count += 1\r\n",
        "        \r\n",
        "        context, question, char_ctx, char_ques, label, ctx_text, ans, ids = batch\r\n",
        "\r\n",
        "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\r\n",
        "                                   char_ctx.to(device), char_ques.to(device), label.to(device)\r\n",
        "\r\n",
        "\r\n",
        "        preds = model(context, question, char_ctx, char_ques)\r\n",
        "\r\n",
        "        start_pred, end_pred = preds\r\n",
        "\r\n",
        "        s_idx, e_idx = label[:,0], label[:,1]\r\n",
        "\r\n",
        "        loss = F.cross_entropy(start_pred, s_idx) + F.cross_entropy(end_pred, e_idx)\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        plot_grad_flow(model.named_parameters())\r\n",
        "        \r\n",
        "        for name, param in model.named_parameters():\r\n",
        "            if(param.requires_grad) and (\"bias\" not in name):\r\n",
        "                writer.add_histogram(name+'_grad',param.grad.abs().mean())\r\n",
        "    \r\n",
        "\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        train_loss += loss.item()\r\n",
        "\r\n",
        "    return train_loss/len(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NhVO-lhfVQt"
      },
      "source": [
        "def valid(model, valid_dataset):\r\n",
        "    \r\n",
        "    print(\"Starting validation .........\")\r\n",
        "   \r\n",
        "    valid_loss = 0.\r\n",
        "\r\n",
        "    batch_count = 0\r\n",
        "    \r\n",
        "    f1, em = 0., 0.\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "        \r\n",
        "   \r\n",
        "    predictions = {}\r\n",
        "    \r\n",
        "    for batch in valid_dataset:\r\n",
        "\r\n",
        "        if batch_count % 500 == 0:\r\n",
        "            print(f\"Starting batch {batch_count}\")\r\n",
        "        batch_count += 1\r\n",
        "\r\n",
        "        context, question, char_ctx, char_ques, label, ctx, answers, ids = batch\r\n",
        "\r\n",
        "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\r\n",
        "                                   char_ctx.to(device), char_ques.to(device), label.to(device)\r\n",
        "        \r\n",
        "       \r\n",
        "\r\n",
        "        \r\n",
        "        with torch.no_grad():\r\n",
        "            \r\n",
        "            s_idx, e_idx = label[:,0], label[:,1]\r\n",
        "\r\n",
        "            preds = model(context, question, char_ctx, char_ques)\r\n",
        "\r\n",
        "            p1, p2 = preds\r\n",
        "\r\n",
        "            \r\n",
        "            loss = F.cross_entropy(p1, s_idx) + F.cross_entropy(p2, e_idx)\r\n",
        "\r\n",
        "            valid_loss += loss.item()\r\n",
        "\r\n",
        "            batch_size, c_len = p1.size()\r\n",
        "            ls = nn.LogSoftmax(dim=1)\r\n",
        "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\r\n",
        "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\r\n",
        "            score, s_idx = score.max(dim=1)\r\n",
        "            score, e_idx = score.max(dim=1)\r\n",
        "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\r\n",
        "            \r\n",
        "           \r\n",
        "            for i in range(batch_size):\r\n",
        "                id = ids[i]\r\n",
        "                pred = context[i][s_idx[i]:e_idx[i]+1]\r\n",
        "                pred = ' '.join([idx2word[idx.item()] for idx in pred])\r\n",
        "                predictions[id] = pred\r\n",
        "            \r\n",
        "\r\n",
        "    \r\n",
        "    em, f1 = evaluate(predictions)\r\n",
        "    return valid_loss/len(valid_dataset), em, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDhlzJH0fVNJ"
      },
      "source": [
        "def evaluate(predictions):\r\n",
        "    '''\r\n",
        "    Gets a dictionary of predictions with question_id as key\r\n",
        "    and prediction as value. The validation dataset has multiple \r\n",
        "    answers for a single question. Hence we compare our prediction\r\n",
        "    with all the answers and choose the one that gives us\r\n",
        "    the maximum metric (em or f1). \r\n",
        "    This method first parses the JSON file, gets all the answers\r\n",
        "    for a given id and then passes the list of answers and the \r\n",
        "    predictions to calculate em, f1.\r\n",
        "    \r\n",
        "    \r\n",
        "    :param dict predictions\r\n",
        "    Returns\r\n",
        "    : exact_match: 1 if the prediction and ground truth \r\n",
        "      match exactly, 0 otherwise.\r\n",
        "    : f1_score: \r\n",
        "    '''\r\n",
        "    with open('./data/squad_dev.json','r',encoding='utf-8') as f:\r\n",
        "        dataset = json.load(f)\r\n",
        "        \r\n",
        "    dataset = dataset['data']\r\n",
        "    f1 = exact_match = total = 0\r\n",
        "    for article in dataset:\r\n",
        "        for paragraph in article['paragraphs']:\r\n",
        "            for qa in paragraph['qas']:\r\n",
        "                total += 1\r\n",
        "                if qa['id'] not in predictions:\r\n",
        "                    continue\r\n",
        "                \r\n",
        "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\r\n",
        "                \r\n",
        "                prediction = predictions[qa['id']]\r\n",
        "                \r\n",
        "                exact_match += metric_max_over_ground_truths(\r\n",
        "                    exact_match_score, prediction, ground_truths)\r\n",
        "                \r\n",
        "                f1 += metric_max_over_ground_truths(\r\n",
        "                    f1_score, prediction, ground_truths)\r\n",
        "                \r\n",
        "    \r\n",
        "    exact_match = 100.0 * exact_match / total\r\n",
        "    f1 = 100.0 * f1 / total\r\n",
        "    \r\n",
        "    return exact_match, f1\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56Ek45Z9fVJs"
      },
      "source": [
        "def normalize_answer(s):\r\n",
        "    '''\r\n",
        "    Performs a series of cleaning steps on the ground truth and \r\n",
        "    predicted answer.\r\n",
        "    '''\r\n",
        "    def remove_articles(text):\r\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\r\n",
        "\r\n",
        "    def white_space_fix(text):\r\n",
        "        return ' '.join(text.split())\r\n",
        "\r\n",
        "    def remove_punc(text):\r\n",
        "        exclude = set(string.punctuation)\r\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\r\n",
        "\r\n",
        "    def lower(text):\r\n",
        "        return text.lower()\r\n",
        "\r\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\r\n",
        "\r\n",
        "\r\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\r\n",
        "    '''\r\n",
        "    Returns maximum value of metrics for predicition by model against\r\n",
        "    multiple ground truths.\r\n",
        "    \r\n",
        "    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\r\n",
        "    :param str prediction: predicted answer span by the model\r\n",
        "    :param list ground_truths: list of ground truths against which\r\n",
        "                               metrics are calculated. Maximum values of \r\n",
        "                               metrics are chosen.\r\n",
        "                            \r\n",
        "    \r\n",
        "    '''\r\n",
        "    scores_for_ground_truths = []\r\n",
        "    for ground_truth in ground_truths:\r\n",
        "        score = metric_fn(prediction, ground_truth)\r\n",
        "        scores_for_ground_truths.append(score)\r\n",
        "        \r\n",
        "    return max(scores_for_ground_truths)\r\n",
        "\r\n",
        "\r\n",
        "def f1_score(prediction, ground_truth):\r\n",
        "    '''\r\n",
        "    Returns f1 score of two strings.\r\n",
        "    '''\r\n",
        "    prediction_tokens = normalize_answer(prediction).split()\r\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\r\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\r\n",
        "    num_same = sum(common.values())\r\n",
        "    if num_same == 0:\r\n",
        "        return 0\r\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\r\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\r\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\r\n",
        "    return f1\r\n",
        "\r\n",
        "\r\n",
        "def exact_match_score(prediction, ground_truth):\r\n",
        "    '''\r\n",
        "    Returns exact_match_score of two strings.\r\n",
        "    '''\r\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\r\n",
        "\r\n",
        "\r\n",
        "def epoch_time(start_time, end_time):\r\n",
        "    '''\r\n",
        "    Helper function to record epoch time.\r\n",
        "    '''\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFG6strffVF7"
      },
      "source": [
        "\r\n",
        "train_losses = []\r\n",
        "valid_losses = []\r\n",
        "ems = []\r\n",
        "f1s = []\r\n",
        "epochs = 5\r\n",
        "for epoch in range(epochs):\r\n",
        "    print(f\"Epoch {epoch+1}\")\r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_dataset)\r\n",
        "    valid_loss, em, f1 = valid(model, valid_dataset)\r\n",
        "    \r\n",
        "    writer.add_scalar('train_loss', train_loss, epoch)\r\n",
        "    writer.add_scalar('valid_loss', valid_loss, epoch)\r\n",
        "    \r\n",
        "    \r\n",
        "    for name, param in model.named_parameters():\r\n",
        "        writer.add_histogram(name, param, epoch)\r\n",
        "    \r\n",
        "    torch.save({\r\n",
        "            'epoch': epoch,\r\n",
        "            'model_state_dict': model.state_dict(),\r\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\r\n",
        "            'loss': valid_loss,\r\n",
        "            'em':em,\r\n",
        "            'f1':f1,\r\n",
        "            }, 'bidaf_run4_{}.pth'.format(epoch))\r\n",
        "    \r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    train_losses.append(train_loss)\r\n",
        "    valid_losses.append(valid_loss)\r\n",
        "    ems.append(em)\r\n",
        "    f1s.append(f1)\r\n",
        "\r\n",
        "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\r\n",
        "    print(f\"Epoch valid loss: {valid_loss}\")\r\n",
        "    print(f\"Epoch EM: {em}\")\r\n",
        "    print(f\"Epoch F1: {f1}\")\r\n",
        "    print(\"====================================================================================\")\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbWiRDdqfVCC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgL66dpzf-CF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoL7vIPSf9-w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgN4DhhcfU_L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOd2PPAIfU8X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdJ7MPX2QMg9"
      },
      "source": [
        "**TO DO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCtU4VhDmLlK"
      },
      "source": [
        "    label_list = []\r\n",
        "    label_list_final = []\r\n",
        "    final_label = []\r\n",
        "    f = open(\"/content/drive/MyDrive/Logic/ScholarlyRead/ScholarlyRead/dev-span.txt\", \"r\") \r\n",
        "    # print(len(f.readlines()))\r\n",
        "    for i in f.read().split('\\n'):\r\n",
        "     \r\n",
        "      s = i.replace('\\t', ',')\r\n",
        "      label_list.append(s)\r\n",
        "      label_list_final = label_list\r\n",
        "\r\n",
        "\r\n",
        "for i in range(len(label_list_final)):\r\n",
        "  k = label_list_final[i].split(\",\")\r\n",
        "  final_label.append(k)\r\n",
        "#\r\n",
        "answer_start = []\r\n",
        "print(final_label)\r\n",
        "# answer_start, answer_end = final_label[0]\r\n",
        "for i in range(len(label_list_final)-1):\r\n",
        "  print(final_label[i])\r\n",
        "  answer_start = final_label[i][0]\r\n",
        "  print(\"s\",answer_start)\r\n",
        "  answer_end = final_label[i][1]\r\n",
        "  print(\"e\",answer_end)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It_kolFlVJ73"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdsmwejumLiU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVCliCBNmLf4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3V4jUkGmIAL"
      },
      "source": [
        "############################################################\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vqk1goe5yu_"
      },
      "source": [
        "# ################################################################\r\n",
        "# #Read label\r\n",
        "# label_list = []\r\n",
        "# label_list_final = []\r\n",
        "# f = open(\"/content/drive/MyDrive/Logic/ScholarlyRead/ScholarlyRead/Train-span.txt\", \"r\")\r\n",
        "# # print(len(f.readlines()))\r\n",
        "# for i in f.read().split('\\n'):\r\n",
        "#   # print(i)\r\n",
        "#   label_list.append(i)\r\n",
        "# label_list = label_list[:-3]\r\n",
        "# print(label_list)\r\n",
        "# # for i in label_list:\r\n",
        "# #   i.replace('\\t', ',')\r\n",
        "# #   label_list_final.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wCdx6S-yVG6"
      },
      "source": [
        "# import csv\r\n",
        "# import pandas as pd\r\n",
        "# import numpy as np\r\n",
        "   \r\n",
        "\r\n",
        "\r\n",
        "# ##################################################################\r\n",
        "# #Read CSV file\r\n",
        "# def get_data_file():\r\n",
        "#     with open(\"/content/drive/MyDrive/Logic/ScholarlyRead/ScholarlyRead/Train.csv\", encoding=\"utf8\") as csv_file:\r\n",
        "#         csv_reader = csv.reader(csv_file)\r\n",
        "#         line_count = []\r\n",
        "#         for row in csv_reader:\r\n",
        "#             line_count.append(row)\r\n",
        "# ####################################################################        \r\n",
        "# context = []\r\n",
        "# question = []\r\n",
        "# Answer = []\r\n",
        "# data = []\r\n",
        "# label = []\r\n",
        "# for i in range(len(line_count)):\r\n",
        "# \ttxt = ' '\r\n",
        "# \tfor j in range(len(line_count[i])):\r\n",
        "# \t\ttxt = txt + line_count[i][j]\r\n",
        "# \tdata.append(txt)\r\n",
        "# \ttxt = txt + '\\n'\r\n",
        "\r\n",
        "\r\n",
        "# ###################################################################\t\r\n",
        "# # Extract the Context Question Answer\r\n",
        "# for i in range(len(line_count)):\r\n",
        "\t\r\n",
        "# \ttest = data[i].strip().split('\\t')\r\n",
        "\t\r\n",
        "# \tcontext.append(test[0])\r\n",
        "# \tquestion.append(test[2])\r\n",
        "# \tAnswer.append(test[1])\r\n",
        "# print(type(context))\r\n",
        "# ####################################################################\r\n",
        "# #Append list in to the DataFrame\t\r\n",
        "\t\t\r\n",
        "# dframe = pd.DataFrame(np.column_stack([context, question, label_list, Answer]), \r\n",
        "#                                columns=['context', 'question', 'label' , 'Answer'])\r\n",
        "\r\n",
        "# # print(dframe.Answer.to_string(index=False))\r\n",
        "# # print(dframe.question.to_string(index=True))\r\n",
        "\r\n",
        "# ##################################################################\r\n",
        "# #delete RoW\r\n",
        "# dframe = dframe.drop([0], axis=0)\r\n",
        "# # print(dframe.head(5))\r\n",
        "# # print(dframe['label'])\r\n",
        "# #################################################################\r\n",
        "# dframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}